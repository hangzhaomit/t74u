{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "require 'torch' -- light my fire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with some Lua under our belts (space suits?) we can get on to getting on to the fun part of designing and training neural networks! Before we can do that, though, we need to learn an array of things about Torch arrays!\n",
    "\n",
    "If you already use a scientific computing platform like SciPy, Julia, R, or [heaven forbid] MATLAB, you'll probably agree that the n-dimensional array is probably the most imporant feature! Torch is no different. Introducing the `Tensor`:\n",
    "\n",
    "# Tensors\n",
    "\n",
    "The aptly-named [Tensor](https://torch7.readthedocs.io/en/latest/tensor/index.html) is Torch's multi-dimensional array data structure which can be created, indexed, sliced, resized, and otherwise modified to suit all of your mathematical needs. These are distinct from tables and are significantly faster and more compact.\n",
    "\n",
    "There's a caveat, however: a Tensor is just a *view* of some bytes somewhere in memory, or the Tensor's `Storage`. Practically, this means is that operations like slicing, indexing, and reshaping are cheap but allocation and resizing are not! This will be important to keep in mind when working on the GPU as allocations are slow and can leak memory.\n",
    "\n",
    "## Creating Tensors\n",
    "\n",
    "The simplest and most generic way of getting a Tensor is to use `torch.Tensor()`. Passing in sizes will automatically allocate (but not initialize!) a `Storage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,.,.) = \n",
       "  6.9520e-310  7.4110e-322   0.0000e+00   0.0000e+00\n",
       "  6.9519e-310  6.9641e+252   8.0341e-95  3.1454e+161\n",
       "  9.9412e-143  5.0462e+180  7.4978e+247  3.8863e+285\n",
       "\n",
       "(2,.,.) = \n",
       "  5.3799e+151  1.6775e+243  4.5481e-144  2.6100e+251\n",
       "  6.1679e+223   1.9657e-62  7.9840e+159  8.0929e+175\n",
       "  5.5622e+180  3.5102e+151  1.3042e-142   1.4427e-71\n",
       "[torch.DoubleTensor of size 2x3x4]\n",
       "\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(2, 3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're so inclined, you can create a Tensor filled with zeros or ones by using `torch.zeros()` and `torch.ones()`, respectively. Additionally, you can pass in your own data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1  2  3\n",
       " 4  5  6\n",
       " 7  8  9\n",
       "[torch.DoubleTensor of size 3x3]\n",
       "\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor{{1, 2, 3}, {4, 5, 6}, {7, 8, 9}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor types\n",
    "\n",
    "\"Wait a second\" you say. \"What's this bit about `torch.DoubleTensor`?\"\n",
    "\n",
    "Another great question! That's the *type* of the Tensor which determines how the rest of Torch interprets the data contained by the `Storage`. Here are some more of the Tensor types that you should know:\n",
    "\n",
    "* `torch.FloatTensor`: 32-bit floating-point (useful for data sent to/received from a 32-bit floating-point GPU)\n",
    "* `torch.LongTensor`: 64-bit integers (useful for holding array indices)\n",
    "* `torch.ByteTensor`: 8-bit unsigned numbers (useful for holding binary masks)\n",
    "* `torch.CudaTensor`: 32-bit fp numbers that have their storage allocated on the GPU\n",
    "* `torch.CudaHalfTensor`: new-fangled type that uses half the memory of fp32 and is faster on some GPUs (e.g., Pascal)\n",
    "\n",
    "Don't concern yourself too much, yet, with the fancy CUDA types. Since we don't have GPUs for this workshop, we won't need them, but they're virtually indispensable when training larger, more complicated models. Figuring out how the CUDA backend works has a slight (read: quite steep) learning curve, so there'll be a guide on this later.\n",
    "\n",
    "The main idea is that certain operations require certain Tensor types; you always convert a Tensor and its Storage to a different type (allocation warning!) by calling `theTensor:<type>()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 26  32   5\n",
       " 30  14  38\n",
       "[torch.LongTensor of size 2x3]\n",
       "\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.rand(2, 3):mul(42):long()) -- converts using floor(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's My Data and I Want it NOW! (View A.G. Tensworth 877...)\n",
    "\n",
    "When we're not crashing dense matrices in multiplications, we usually only want to deal with small segments of our data. For this, all we must do is create new Tensors that only look at a portion of the original Storage.\n",
    "\n",
    "To help introduce the next concepts, we'll need a volunteer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  1   2   3   4\n",
       "  5   6   7   8\n",
       "  9  10  11  12\n",
       "[torch.DoubleTensor of size 3x4]\n",
       "\n",
       " 3\n",
       " 4\n",
       "[torch.LongStorage of size 2]\n",
       "\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "friend = torch.Tensor(3, 4):range(1, 12)\n",
    "print(friend, friend:size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing and Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  7\n",
       " 11\n",
       "[torch.DoubleTensor of size 2]\n",
       "\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row1 = friend[3]\n",
    "col2 = friend[{{}, 2}]\n",
    "eight = friend[{2, 4}]\n",
    "sevenEleven = friend[{{2,3}, 3}]\n",
    "print(sevenEleven)\n",
    "assert(friend:size(1) == 3 and friend:size(2) == 4) -- friend remains unchanged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're coming from one of the other scientific computing platforms, you'll notice the similarity between an inner table and the `:` operator.\n",
    "\n",
    "While specifying indices using tables is convenient and all, sometimes it's easier to directly request the data you want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row1 = friend:select(1, 1) -- select(dimension, index along that dimension)\n",
    "col2 = friend:select(2, 2)\n",
    "sevenEleven = friend:sub(2, 3, 3, 3) -- sub(from1, to1, from2, to2, ...)\n",
    "alsoSevenEleven = friend:narrow(1, 2, 2):narrow(2, 3, 1) -- narrow(dimension, startIdx, sliceLen)\n",
    "assert(sevenEleven:equal(alsoSevenEleven))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment\n",
    "\n",
    "Not bad! Now that we can tell Torch which indices we want, we can start to get choosy with their contents.\n",
    "\n",
    "When using the convenient table syntax, the `=` operator is overloaded to do assignment. If, instead, you go the route of `select`, `narrow`, and `sub`, you'll have to use the likes of `copy` and `fill`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0   2   3   4\n",
       "  5   6   7   8\n",
       "  9  10  11  12\n",
       "[torch.DoubleTensor of size 3x4]\n",
       "\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oldFriend = friend:clone() -- forgotten but not gone\n",
    "friend[{1, 1}] = 0\n",
    "print(friend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0  2  3  4\n",
       " 5  6  7  8\n",
       "-1 -2 -3 -4\n",
       "[torch.DoubleTensor of size 3x4]\n",
       "\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "friend:select(1, 3):copy(oldFriend[1]):mul(-1)\n",
    "print(friend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0  2  3  4\n",
       " 5  0  0  8\n",
       "-1 -2 -3 -4\n",
       "[torch.DoubleTensor of size 3x4]\n",
       "\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "friend:narrow(2, 2, 2):select(1, 2):zero()\n",
    "print(friend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presented above was only a selected subset (no pun intended) of the full Tensor indexing API. You will want to familiarize yourself with [the rest of the commands](https://torch7.readthedocs.io/en/latest/tensor/index.html) in order to write maximally terse and performant code!\n",
    "\n",
    "## Tensor Lifecycle\n",
    "\n",
    "When training neural networks on the GPU, the life of a Tensor looks a bit like this\n",
    "\n",
    "1. Create a Tensor in RAM\n",
    "2. Fill the Tensor with a batch of data\n",
    "3. Copy the Tensor to a pre-allocated counterpart on the GPU\n",
    "4. Trash the RAM Tensor\n",
    "\n",
    "It's kind of like a bucket brigade except with numbers for water, GPUs for burning buildings, and math for fire!\n",
    "\"Gee, I can't imagine how lit this guy must have been when he came up with that one...\" you think to yourself.  \n",
    "Well, let's just say that I have a few models training while I'm writing this.  \n",
    "I bet you're really confused now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎉 More Congratulations! 🎉\n",
    "\n",
    "Hooray! You now know how to make pretty, custom Tensors.\n",
    "\n",
    "We totally skipped over all of the math stuff you can do with Tensors, but we'll be doing that in the next part, the best part, the *net* part!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
